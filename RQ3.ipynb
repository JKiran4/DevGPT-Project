{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DevGPT/snapshot_20230727/20230727_195816_hn_sharings.json', 'DevGPT/snapshot_20230727/20230727_195927_pr_sharings.json', 'DevGPT/snapshot_20230727/20230727_195941_issue_sharings.json', 'DevGPT/snapshot_20230727/20230727_195954_discussion_sharings.json', 'DevGPT/snapshot_20230727/20230727_200003_commit_sharings.json', 'DevGPT/snapshot_20230727/20230727_200102_file_sharings.json', 'DevGPT/snapshot_20230803/20230803_093947_pr_sharings.json', 'DevGPT/snapshot_20230803/20230803_094705_issue_sharings.json', 'DevGPT/snapshot_20230803/20230803_094811_discussion_sharings.json', 'DevGPT/snapshot_20230803/20230803_095317_commit_sharings.json', 'DevGPT/snapshot_20230803/20230803_103605_file_sharings.json', 'DevGPT/snapshot_20230803/20230803_105332_hn_sharings.json', 'DevGPT/snapshot_20230810/20230810_123110_pr_sharings.json', 'DevGPT/snapshot_20230810/20230810_123938_issue_sharings.json', 'DevGPT/snapshot_20230810/20230810_124048_discussion_sharings.json', 'DevGPT/snapshot_20230810/20230810_124807_commit_sharings.json', 'DevGPT/snapshot_20230810/20230810_133121_file_sharings.json', 'DevGPT/snapshot_20230810/20230810_134011_hn_sharings.json', 'DevGPT/snapshot_20230817/20230817_125147_pr_sharings.json', 'DevGPT/snapshot_20230817/20230817_130502_issue_sharings.json', 'DevGPT/snapshot_20230817/20230817_130721_discussion_sharings.json', 'DevGPT/snapshot_20230817/20230817_131244_commit_sharings.json', 'DevGPT/snapshot_20230817/20230817_151344_file_sharings.json', 'DevGPT/snapshot_20230817/20230817_170022_hn_sharings.json', 'DevGPT/snapshot_20230824/20230824_100450_pr_sharings.json', 'DevGPT/snapshot_20230824/20230824_101836_issue_sharings.json', 'DevGPT/snapshot_20230824/20230824_102000_discussion_sharings.json', 'DevGPT/snapshot_20230824/20230824_102435_commit_sharings.json', 'DevGPT/snapshot_20230824/20230824_111114_file_sharings.json', 'DevGPT/snapshot_20230824/20230824_112153_hn_sharings.json', 'DevGPT/snapshot_20230831/20230831_060603_pr_sharings.json', 'DevGPT/snapshot_20230831/20230831_061759_issue_sharings.json', 'DevGPT/snapshot_20230831/20230831_061926_discussion_sharings.json', 'DevGPT/snapshot_20230831/20230831_063412_commit_sharings.json', 'DevGPT/snapshot_20230831/20230831_072722_file_sharings.json', 'DevGPT/snapshot_20230831/20230831_073827_hn_sharings.json', 'DevGPT/snapshot_20230907/20230907_091631_pr_sharings.json', 'DevGPT/snapshot_20230907/20230907_092956_issue_sharings.json', 'DevGPT/snapshot_20230907/20230907_093129_discussion_sharings.json', 'DevGPT/snapshot_20230907/20230907_110036_commit_sharings.json', 'DevGPT/snapshot_20230907/20230907_121304_file_sharings.json', 'DevGPT/snapshot_20230907/20230907_123434_hn_sharings.json', 'DevGPT/snapshot_20230914/20230914_074826_pr_sharings.json', 'DevGPT/snapshot_20230914/20230914_080417_issue_sharings.json', 'DevGPT/snapshot_20230914/20230914_080601_discussion_sharings.json', 'DevGPT/snapshot_20230914/20230914_083202_commit_sharings.json', 'DevGPT/snapshot_20230914/20230914_104122_file_sharings.json', 'DevGPT/snapshot_20230914/20230914_105439_hn_sharings.json', 'DevGPT/snapshot_20231012/20231012_230826_commit_sharings.json', 'DevGPT/snapshot_20231012/20231012_232232_hn_sharings.json', 'DevGPT/snapshot_20231012/20231012_233628_pr_sharings.json', 'DevGPT/snapshot_20231012/20231012_234250_file_sharings.json', 'DevGPT/snapshot_20231012/20231012_235128_issue_sharings.json', 'DevGPT/snapshot_20231012/20231012_235320_discussion_sharings.json']\n"
     ]
    }
   ],
   "source": [
    "# don't run this every time! re-import the exported csv to save time.\n",
    "# commented out below to prevent accidental execution.\n",
    "\n",
    "# dirs = [(\"DevGPT/\" + d) for d in os.listdir(\"DevGPT\") if os.path.isdir(\"DevGPT/\" + d)] # modify this to limit snapshots\n",
    "\n",
    "# files = []\n",
    "# for dir in dirs:\n",
    "#     for f in os.listdir(dir):\n",
    "#         if f[-5:] == \".json\": files.append(dir + \"/\" + f)\n",
    "\n",
    "# print(files)\n",
    "\n",
    "# imports = []\n",
    "# for f in files:\n",
    "#     if f[-5:] == \".json\":  # this one pulls all files\n",
    "#         trydf = pd.read_json(f)\n",
    "#         trydf = pd.json_normalize(trydf[\"Sources\"])\n",
    "#         trydf[\"filepath\"] = f\n",
    "#         imports.append(trydf)\n",
    "\n",
    "# df = pd.concat(imports, ignore_index = True)\n",
    "# df.to_csv(\"df-export.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Local\\Temp\\ipykernel_7784\\848057934.py:1: DtypeWarning: Columns (5,7,17,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfi = pd.read_csv(\"df-export.csv\")\n"
     ]
    }
   ],
   "source": [
    "dfi = pd.read_csv(\"df-export.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Title   Body  Message  CommitMessage\n",
      "0   True  False    False          False\n",
      "     Title  Body  Message  CommitMessage\n",
      "138   True  True    False          False\n",
      "     Title  Body  Message  CommitMessage\n",
      "285   True  True    False          False\n",
      "     Title  Body  Message  CommitMessage\n",
      "520   True  True    False          False\n",
      "     Title   Body  Message  CommitMessage\n",
      "552  False  False     True          False\n",
      "     Title   Body  Message  CommitMessage\n",
      "731  False  False    False           True\n"
     ]
    }
   ],
   "source": [
    "dfa = dfi[[\"Type\", \"Author\", \"Title\", \"Body\", \"Message\", \"CommitMessage\", \"filepath\"]]\n",
    "\n",
    "# query statements, checking which columns have text to extract\n",
    "\n",
    "hn_q = \"filepath.str.contains('hn_sharings.json')\"     # Title\n",
    "pr_q = \"filepath.str.contains('pr_sharings.json')\"     # Title, Body\n",
    "is_q = \"filepath.str.contains('issue_sharings.json')\"     # Title, Body\n",
    "di_q = \"filepath.str.contains('discussion_sharings.json')\"    # Title, Body\n",
    "co_q = \"filepath.str.contains('commit_sharings.json')\"         # Message\n",
    "fi_q = \"filepath.str.contains('file_sharings.json')\"          # CommitMessage\n",
    "\n",
    "for query in [hn_q, pr_q, is_q, di_q, co_q, fi_q]:\n",
    "    print(~dfa.query(query)[[\"Title\", \"Body\", \"Message\", \"CommitMessage\"]].head(1).isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Local\\Temp\\ipykernel_7784\\3042202609.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfa[\"text\"] = dfa[[\"Title\", \"Body\", \"Message\", \"CommitMessage\"]].agg(lambda x: \"\".join(x.dropna().astype(str)), axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cbowal</td>\n",
       "      <td>OpenAI shuts down its AI Classifier due to poo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>warrenm</td>\n",
       "      <td>“Devil’s horsemen”: Why Mongol horse archers w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>ayhanfuat</td>\n",
       "      <td>The Fall of Stack Overflow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cdme</td>\n",
       "      <td>Death Metal English (2013)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>notRobot</td>\n",
       "      <td>Shopify employee breaks NDA to reveal firm rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>discussion</td>\n",
       "      <td>l1t1</td>\n",
       "      <td>how to install on proot ubuntu of termux```\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18710</th>\n",
       "      <td>discussion</td>\n",
       "      <td>rattrayalex</td>\n",
       "      <td>v3 to v4 Migration Guidev4 is a complete rewri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>discussion</td>\n",
       "      <td>lukecookssw</td>\n",
       "      <td>2023-06-9 - Databases, data warehouses, and da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>discussion</td>\n",
       "      <td>ZoeLeBlanc</td>\n",
       "      <td>AI &amp; CLI AssignmentPost the results of your as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18713</th>\n",
       "      <td>discussion</td>\n",
       "      <td>MrAAafa99</td>\n",
       "      <td>Exploring the Plotting of Pair Correlation Fun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18714 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type       Author  \\\n",
       "0      hacker news       cbowal   \n",
       "1      hacker news      warrenm   \n",
       "2      hacker news    ayhanfuat   \n",
       "3      hacker news         cdme   \n",
       "4      hacker news     notRobot   \n",
       "...            ...          ...   \n",
       "18709   discussion         l1t1   \n",
       "18710   discussion  rattrayalex   \n",
       "18711   discussion  lukecookssw   \n",
       "18712   discussion   ZoeLeBlanc   \n",
       "18713   discussion    MrAAafa99   \n",
       "\n",
       "                                                    text  \n",
       "0      OpenAI shuts down its AI Classifier due to poo...  \n",
       "1      “Devil’s horsemen”: Why Mongol horse archers w...  \n",
       "2                             The Fall of Stack Overflow  \n",
       "3                             Death Metal English (2013)  \n",
       "4      Shopify employee breaks NDA to reveal firm rep...  \n",
       "...                                                  ...  \n",
       "18709  how to install on proot ubuntu of termux```\\r\\...  \n",
       "18710  v3 to v4 Migration Guidev4 is a complete rewri...  \n",
       "18711  2023-06-9 - Databases, data warehouses, and da...  \n",
       "18712  AI & CLI AssignmentPost the results of your as...  \n",
       "18713  Exploring the Plotting of Pair Correlation Fun...  \n",
       "\n",
       "[18714 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine text columns\n",
    "\n",
    "dfa[\"text\"] = dfa[[\"Title\", \"Body\", \"Message\", \"CommitMessage\"]].agg(lambda x: \"\".join(x.dropna().astype(str)), axis = 1)\n",
    "dfa = dfa.drop([\"Title\", \"Body\", \"Message\", \"CommitMessage\", \"filepath\"], axis = 1)\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Author</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cbowal</td>\n",
       "      <td>[openai, shuts, down, its, ai, classifier, due...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>warrenm</td>\n",
       "      <td>[devil, horsemen, why, mongol, horse, archers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>ayhanfuat</td>\n",
       "      <td>[the, fall, of, stack, overflow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cdme</td>\n",
       "      <td>[death, metal, english]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>notRobot</td>\n",
       "      <td>[shopify, employee, breaks, nda, to, reveal, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>discussion</td>\n",
       "      <td>l1t1</td>\n",
       "      <td>[how, to, install, on, proot, ubuntu, of, term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18710</th>\n",
       "      <td>discussion</td>\n",
       "      <td>rattrayalex</td>\n",
       "      <td>[to, migration, guidev, is, complete, rewrite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>discussion</td>\n",
       "      <td>lukecookssw</td>\n",
       "      <td>[databases, data, warehouses, and, data, lakes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>discussion</td>\n",
       "      <td>ZoeLeBlanc</td>\n",
       "      <td>[ai, cli, assignmentpost, the, results, of, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18713</th>\n",
       "      <td>discussion</td>\n",
       "      <td>MrAAafa99</td>\n",
       "      <td>[exploring, the, plotting, of, pair, correlati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18714 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type       Author  \\\n",
       "0      hacker news       cbowal   \n",
       "1      hacker news      warrenm   \n",
       "2      hacker news    ayhanfuat   \n",
       "3      hacker news         cdme   \n",
       "4      hacker news     notRobot   \n",
       "...            ...          ...   \n",
       "18709   discussion         l1t1   \n",
       "18710   discussion  rattrayalex   \n",
       "18711   discussion  lukecookssw   \n",
       "18712   discussion   ZoeLeBlanc   \n",
       "18713   discussion    MrAAafa99   \n",
       "\n",
       "                                                 text_pp  \n",
       "0      [openai, shuts, down, its, ai, classifier, due...  \n",
       "1      [devil, horsemen, why, mongol, horse, archers,...  \n",
       "2                       [the, fall, of, stack, overflow]  \n",
       "3                                [death, metal, english]  \n",
       "4      [shopify, employee, breaks, nda, to, reveal, f...  \n",
       "...                                                  ...  \n",
       "18709  [how, to, install, on, proot, ubuntu, of, term...  \n",
       "18710  [to, migration, guidev, is, complete, rewrite,...  \n",
       "18711  [databases, data, warehouses, and, data, lakes...  \n",
       "18712  [ai, cli, assignmentpost, the, results, of, yo...  \n",
       "18713  [exploring, the, plotting, of, pair, correlati...  \n",
       "\n",
       "[18714 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa[\"text_pp\"] = dfa[\"text\"].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "dfa = dfa.drop([\"text\"], axis = 1)\n",
    "\n",
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1883"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa[\"Author\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply model - basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy: 86.91 %\n"
     ]
    }
   ],
   "source": [
    "# split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfa[\"text_pp\"], dfa[\"Author\"] , test_size = 0.2, random_state = 2)\n",
    "\n",
    "# apply word vector model on training data\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size = 100,\n",
    "                                   window = 5,\n",
    "                                   min_count = 2)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "# create average sentence vectors to train on\n",
    "X_train_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_train])\n",
    "\n",
    "X_test_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_test])\n",
    "\n",
    "# define random forest model\n",
    "rf = RandomForestClassifier(n_estimators = 100, random_state = 2)\n",
    "\n",
    "# fit rf model to vectorized training data\n",
    "rf.fit(X_train_vect, y_train)\n",
    "\n",
    "# predict values on vectorized test data\n",
    "y_pred = rf.predict(X_test_vect)\n",
    "\n",
    "# report accuracy of test data predictions\n",
    "print(\"Test set classification accuracy: %.2f %%\" % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply model - optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18714 rows in dataframe for authors with 1 or more submissions -- 100.0 % of original.\n",
      "18320 rows in dataframe for authors with 2 or more submissions -- 97.9 % of original.\n",
      "17217 rows in dataframe for authors with 5 or more submissions -- 92.0 % of original.\n",
      "10553 rows in dataframe for authors with 10 or more submissions -- 56.4 % of original.\n",
      "5146 rows in dataframe for authors with 100 or more submissions -- 27.5 % of original.\n"
     ]
    }
   ],
   "source": [
    "# checking how many rows are removed for minimum comment thresholds\n",
    "for i in [1, 2, 5, 10, 100]:\n",
    "    count = len(dfa.groupby(\"Author\").filter(lambda x: len(x) >= i))\n",
    "    perc = count / len(dfa) * 100\n",
    "    print(\"%i rows in dataframe for authors with %i or more submissions -- %.1f %% of original.\" %\n",
    "          (count, i, perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Author</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cbowal</td>\n",
       "      <td>[openai, shuts, down, its, ai, classifier, due...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>warrenm</td>\n",
       "      <td>[devil, horsemen, why, mongol, horse, archers,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>ayhanfuat</td>\n",
       "      <td>[the, fall, of, stack, overflow, hacker news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>cdme</td>\n",
       "      <td>[death, metal, english, hacker news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hacker news</td>\n",
       "      <td>notRobot</td>\n",
       "      <td>[shopify, employee, breaks, nda, to, reveal, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18708</th>\n",
       "      <td>discussion</td>\n",
       "      <td>kyzer-davis</td>\n",
       "      <td>[name, for, uuid, with, all, bits, set, to, ff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18709</th>\n",
       "      <td>discussion</td>\n",
       "      <td>l1t1</td>\n",
       "      <td>[how, to, install, on, proot, ubuntu, of, term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18710</th>\n",
       "      <td>discussion</td>\n",
       "      <td>rattrayalex</td>\n",
       "      <td>[to, migration, guidev, is, complete, rewrite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18711</th>\n",
       "      <td>discussion</td>\n",
       "      <td>lukecookssw</td>\n",
       "      <td>[databases, data, warehouses, and, data, lakes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18713</th>\n",
       "      <td>discussion</td>\n",
       "      <td>MrAAafa99</td>\n",
       "      <td>[exploring, the, plotting, of, pair, correlati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17217 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Type       Author  \\\n",
       "0      hacker news       cbowal   \n",
       "1      hacker news      warrenm   \n",
       "2      hacker news    ayhanfuat   \n",
       "3      hacker news         cdme   \n",
       "4      hacker news     notRobot   \n",
       "...            ...          ...   \n",
       "18708   discussion  kyzer-davis   \n",
       "18709   discussion         l1t1   \n",
       "18710   discussion  rattrayalex   \n",
       "18711   discussion  lukecookssw   \n",
       "18713   discussion    MrAAafa99   \n",
       "\n",
       "                                                 text_pp  \n",
       "0      [openai, shuts, down, its, ai, classifier, due...  \n",
       "1      [devil, horsemen, why, mongol, horse, archers,...  \n",
       "2          [the, fall, of, stack, overflow, hacker news]  \n",
       "3                   [death, metal, english, hacker news]  \n",
       "4      [shopify, employee, breaks, nda, to, reveal, f...  \n",
       "...                                                  ...  \n",
       "18708  [name, for, uuid, with, all, bits, set, to, ff...  \n",
       "18709  [how, to, install, on, proot, ubuntu, of, term...  \n",
       "18710  [to, migration, guidev, is, complete, rewrite,...  \n",
       "18711  [databases, data, warehouses, and, data, lakes...  \n",
       "18713  [exploring, the, plotting, of, pair, correlati...  \n",
       "\n",
       "[17217 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove text from authors that have fewer than 5 comments\n",
    "dfb = dfa.groupby(\"Author\").filter(lambda x: len(x) >= 5)\n",
    "\n",
    "# include post type as a token \n",
    "dfb[\"text_pp\"] = dfb.apply(lambda row: row[\"text_pp\"] + [row[\"Type\"]], axis = 1)\n",
    "\n",
    "dfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfb[\"Author\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning w2v_model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 50 and window size 5:\n",
      "Classification accuracy: 0.918 , std dev: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 50 and window size 10:\n",
      "Classification accuracy: 0.917 , std dev: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 50 and window size 15:\n",
      "Classification accuracy: 0.918 , std dev: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 100 and window size 5:\n",
      "Classification accuracy: 0.918 , std dev: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 100 and window size 10:\n",
      "Classification accuracy: 0.919 , std dev: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 100 and window size 15:\n",
      "Classification accuracy: 0.918 , std dev: 0.003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 200 and window size 5:\n",
      "Classification accuracy: 0.918 , std dev: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 200 and window size 10:\n",
      "Classification accuracy: 0.918 , std dev: 0.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model with vector size 200 and window size 15:\n",
      "Classification accuracy: 0.918 , std dev: 0.004\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfb[\"text_pp\"], dfb[\"Author\"] , test_size = 0.2, random_state = 2) \n",
    "# included Type column in text column this time for additional predictive power\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "\n",
    "for vec, win in [[50, 5], [50, 10], [50, 15],\n",
    "                 [100, 5], [100, 10], [100, 15],\n",
    "                 [200, 5], [200, 10], [200, 15]]:\n",
    "\n",
    "    scores = []\n",
    "    for train_index, test_index in skf.split(X_train, y_train):\n",
    "\n",
    "        X_tr = X_train.iloc[train_index]\n",
    "        X_te = X_train.iloc[test_index]\n",
    "        y_tr = y_train.iloc[train_index]\n",
    "        y_te = y_train.iloc[test_index]\n",
    "\n",
    "        w2v_model = gensim.models.Word2Vec(X_tr,\n",
    "                                        vector_size = vec,\n",
    "                                        window = win,\n",
    "                                        min_count = 1)\n",
    "        # not too much extra to include all words: 12394 for count 1, 11685 for count 2 (default)\n",
    "\n",
    "        words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "        X_train_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                                if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                                for ls in X_tr])\n",
    "\n",
    "        X_test_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                                if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                                for ls in X_te])\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators = 100, random_state = 2)\n",
    "\n",
    "        # fit rf model to vectorized training data\n",
    "        rf.fit(X_train_vect, y_tr)\n",
    "\n",
    "        # predict values on vectorized test data\n",
    "        y_pred = rf.predict(X_test_vect)\n",
    "\n",
    "        scores.append(accuracy_score(y_te, y_pred))\n",
    "    # report accuracy of test data predictions\n",
    "    print(\"For model with vector size %i and window size %i:\\nClassification accuracy: %.3f , std dev: %.3f\" % (vec, win, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seamu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\model_selection\\_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy: 92.33 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfb[\"text_pp\"], dfb[\"Author\"] , test_size = 0.2, random_state = 2) \n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size = 100,\n",
    "                                   window = 10,\n",
    "                                   min_count = 1)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "X_train_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_train])\n",
    "\n",
    "X_test_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_test])\n",
    "\n",
    "# define possible hyperparameters to tune\n",
    "rf_params = {\"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "             \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "             \"n_estimators\": [10, 20, 50, 100],\n",
    "           \"max_depth\": range(1, 20)}\n",
    "\n",
    "rf_tuned = RandomizedSearchCV(RandomForestClassifier(), rf_params, n_iter = 10, cv = skf,\n",
    "                               random_state = 2, verbose = 1)\n",
    "\n",
    "rf_tuned.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = rf_tuned.predict(X_test_vect)\n",
    "\n",
    "print(\"Test set classification accuracy: %.2f %%\" % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9182454822353758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 13,\n",
       " 'criterion': 'log_loss'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rf_tuned.best_score_)\n",
    "rf_tuned.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Test set classification accuracy: 97.60 %\n"
     ]
    }
   ],
   "source": [
    "# try to predict category?\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfa[\"text_pp\"], dfa[\"Type\"] , test_size = 0.2, random_state = 2) \n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                   vector_size = 50,\n",
    "                                   window = 10,\n",
    "                                   min_count = 1)\n",
    "\n",
    "words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "X_train_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_train])\n",
    "\n",
    "X_test_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                         if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                         for ls in X_test])\n",
    "\n",
    "# define possible hyperparameters to tune\n",
    "rf_params = {\"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "             \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "             \"n_estimators\": [10, 20, 50, 100],\n",
    "           \"max_depth\": range(1, 20)}\n",
    "\n",
    "rf_tuned = RandomizedSearchCV(RandomForestClassifier(), rf_params, n_iter = 10, cv = skf,\n",
    "                               random_state = 2, verbose = 1)\n",
    "\n",
    "rf_tuned.fit(X_train_vect, y_train)\n",
    "\n",
    "y_pred = rf_tuned.predict(X_test_vect)\n",
    "\n",
    "print(\"Test set classification accuracy: %.2f %%\" % (accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set classification accuracy for users with 1 or more posts: 87.15 %\n",
      "Test set classification accuracy for users with 3 or more posts: 90.69 %\n",
      "Test set classification accuracy for users with 5 or more posts: 92.36 %\n",
      "Test set classification accuracy for users with 7 or more posts: 92.91 %\n",
      "Test set classification accuracy for users with 9 or more posts: 94.23 %\n",
      "Test set classification accuracy for users with 11 or more posts: 95.12 %\n",
      "Test set classification accuracy for users with 13 or more posts: 95.67 %\n",
      "Test set classification accuracy for users with 15 or more posts: 96.17 %\n",
      "Test set classification accuracy for users with 17 or more posts: 96.11 %\n",
      "Test set classification accuracy for users with 19 or more posts: 97.16 %\n",
      "Test set classification accuracy for users with 21 or more posts: 97.98 %\n",
      "Test set classification accuracy for users with 23 or more posts: 97.46 %\n",
      "Test set classification accuracy for users with 25 or more posts: 98.19 %\n",
      "Test set classification accuracy for users with 27 or more posts: 98.16 %\n",
      "Test set classification accuracy for users with 29 or more posts: 98.05 %\n",
      "Test set classification accuracy for users with 31 or more posts: 98.73 %\n",
      "Test set classification accuracy for users with 33 or more posts: 97.93 %\n",
      "Test set classification accuracy for users with 35 or more posts: 98.47 %\n",
      "Test set classification accuracy for users with 37 or more posts: 98.45 %\n",
      "Test set classification accuracy for users with 39 or more posts: 98.88 %\n",
      "Test set classification accuracy for users with 41 or more posts: 99.38 %\n",
      "Test set classification accuracy for users with 43 or more posts: 99.38 %\n",
      "Test set classification accuracy for users with 45 or more posts: 98.96 %\n",
      "Test set classification accuracy for users with 47 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 49 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 51 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 53 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 55 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 57 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 59 or more posts: 99.59 %\n",
      "Test set classification accuracy for users with 61 or more posts: 99.67 %\n",
      "Test set classification accuracy for users with 63 or more posts: 99.75 %\n",
      "Test set classification accuracy for users with 65 or more posts: 99.75 %\n",
      "Test set classification accuracy for users with 67 or more posts: 99.75 %\n",
      "Test set classification accuracy for users with 69 or more posts: 99.75 %\n",
      "Test set classification accuracy for users with 71 or more posts: 99.83 %\n",
      "Test set classification accuracy for users with 73 or more posts: 99.66 %\n",
      "Test set classification accuracy for users with 75 or more posts: 99.74 %\n",
      "Test set classification accuracy for users with 77 or more posts: 99.57 %\n",
      "Test set classification accuracy for users with 79 or more posts: 99.31 %\n",
      "Test set classification accuracy for users with 81 or more posts: 99.39 %\n",
      "Test set classification accuracy for users with 83 or more posts: 99.31 %\n",
      "Test set classification accuracy for users with 85 or more posts: 99.63 %\n",
      "Test set classification accuracy for users with 87 or more posts: 99.72 %\n",
      "Test set classification accuracy for users with 89 or more posts: 99.45 %\n",
      "Test set classification accuracy for users with 91 or more posts: 99.81 %\n",
      "Test set classification accuracy for users with 93 or more posts: 99.62 %\n",
      "Test set classification accuracy for users with 95 or more posts: 99.81 %\n",
      "Test set classification accuracy for users with 97 or more posts: 99.71 %\n",
      "Test set classification accuracy for users with 99 or more posts: 99.61 %\n"
     ]
    }
   ],
   "source": [
    "minposts = []\n",
    "accuracies = []\n",
    "\n",
    "for min_posts in range(1, 101, 2):\n",
    "    dfc = dfa.groupby(\"Author\").filter(lambda x: len(x) >= min_posts)\n",
    "    dfc[\"text_pp\"] = dfc.apply(lambda row: row[\"text_pp\"] + [row[\"Type\"]], axis = 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    dfc[\"text_pp\"], dfc[\"Author\"] , test_size = 0.2, random_state = 2) \n",
    "\n",
    "    w2v_model = gensim.models.Word2Vec(X_train,\n",
    "                                    vector_size = 100,\n",
    "                                    window = 10,\n",
    "                                    min_count = 1)\n",
    "\n",
    "    words = set(w2v_model.wv.index_to_key)\n",
    "\n",
    "    X_train_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                            if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                            for ls in X_train])\n",
    "\n",
    "    X_test_vect = np.array([np.mean([w2v_model.wv[i] for i in ls if i in words], axis=0) \n",
    "                            if any(i in words for i in ls) else np.zeros(w2v_model.vector_size)\n",
    "                            for ls in X_test])\n",
    "\n",
    "    rfc = RandomForestClassifier(n_estimators = 100, criterion = \"log_loss\", max_depth = 13, max_features = \"sqrt\")\n",
    "\n",
    "    rfc.fit(X_train_vect, y_train)\n",
    "\n",
    "    y_pred = rfc.predict(X_test_vect)\n",
    "\n",
    "    minposts.append(min_posts)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred) * 100)\n",
    "\n",
    "    # print(\"Test set classification accuracy for users with %i or more posts: %.2f %%\" % (min_posts, accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25.vega-embed details,\n",
       "  #altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-52f36cb923e04a1d90aa82cd6f2d6e25\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 16, \"titleFontSize\": 16}}, \"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"grey\"}, \"encoding\": {\"x\": {\"field\": \"Minimum posts\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Accuracy\", \"scale\": {\"zero\": false}, \"title\": \"Accuracy (%)\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"point\", \"fill\": \"black\", \"stroke\": \"black\"}, \"encoding\": {\"x\": {\"field\": \"Minimum posts\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Accuracy\", \"scale\": {\"zero\": false}, \"title\": \"Accuracy (%)\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-09c679183f2824a67556f11be1f05ff3\"}, \"height\": 200, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-09c679183f2824a67556f11be1f05ff3\": [{\"Minimum posts\": 1, \"Accuracy\": 87.14934544483035}, {\"Minimum posts\": 3, \"Accuracy\": 90.69444444444444}, {\"Minimum posts\": 5, \"Accuracy\": 92.36353077816493}, {\"Minimum posts\": 7, \"Accuracy\": 92.91485394655066}, {\"Minimum posts\": 9, \"Accuracy\": 94.22680412371133}, {\"Minimum posts\": 11, \"Accuracy\": 95.11841469308845}, {\"Minimum posts\": 13, \"Accuracy\": 95.66517189835575}, {\"Minimum posts\": 15, \"Accuracy\": 96.1677887105127}, {\"Minimum posts\": 17, \"Accuracy\": 96.11440906637884}, {\"Minimum posts\": 19, \"Accuracy\": 97.15976331360947}, {\"Minimum posts\": 21, \"Accuracy\": 97.98288508557457}, {\"Minimum posts\": 23, \"Accuracy\": 97.45972738537795}, {\"Minimum posts\": 25, \"Accuracy\": 98.19121447028424}, {\"Minimum posts\": 27, \"Accuracy\": 98.15546772068511}, {\"Minimum posts\": 29, \"Accuracy\": 98.05013927576601}, {\"Minimum posts\": 31, \"Accuracy\": 98.72611464968153}, {\"Minimum posts\": 33, \"Accuracy\": 97.92857142857143}, {\"Minimum posts\": 35, \"Accuracy\": 98.47050254916242}, {\"Minimum posts\": 37, \"Accuracy\": 98.44674556213018}, {\"Minimum posts\": 39, \"Accuracy\": 98.87808526551981}, {\"Minimum posts\": 41, \"Accuracy\": 99.38319198149576}, {\"Minimum posts\": 43, \"Accuracy\": 99.37548790007807}, {\"Minimum posts\": 45, \"Accuracy\": 98.95666131621188}, {\"Minimum posts\": 47, \"Accuracy\": 99.5928338762215}, {\"Minimum posts\": 49, \"Accuracy\": 99.5928338762215}, {\"Minimum posts\": 51, \"Accuracy\": 99.58609271523179}, {\"Minimum posts\": 53, \"Accuracy\": 99.58609271523179}, {\"Minimum posts\": 55, \"Accuracy\": 99.58609271523179}, {\"Minimum posts\": 57, \"Accuracy\": 99.58609271523179}, {\"Minimum posts\": 59, \"Accuracy\": 99.58609271523179}, {\"Minimum posts\": 61, \"Accuracy\": 99.66555183946488}, {\"Minimum posts\": 63, \"Accuracy\": 99.74916387959865}, {\"Minimum posts\": 65, \"Accuracy\": 99.74916387959865}, {\"Minimum posts\": 67, \"Accuracy\": 99.74916387959865}, {\"Minimum posts\": 69, \"Accuracy\": 99.74916387959865}, {\"Minimum posts\": 71, \"Accuracy\": 99.83277591973244}, {\"Minimum posts\": 73, \"Accuracy\": 99.65724078834619}, {\"Minimum posts\": 75, \"Accuracy\": 99.74293059125964}, {\"Minimum posts\": 77, \"Accuracy\": 99.57155098543274}, {\"Minimum posts\": 79, \"Accuracy\": 99.30555555555556}, {\"Minimum posts\": 81, \"Accuracy\": 99.39236111111111}, {\"Minimum posts\": 83, \"Accuracy\": 99.30555555555556}, {\"Minimum posts\": 85, \"Accuracy\": 99.63133640552996}, {\"Minimum posts\": 87, \"Accuracy\": 99.72350230414746}, {\"Minimum posts\": 89, \"Accuracy\": 99.44700460829493}, {\"Minimum posts\": 91, \"Accuracy\": 99.81255857544518}, {\"Minimum posts\": 93, \"Accuracy\": 99.61868446139181}, {\"Minimum posts\": 95, \"Accuracy\": 99.80934223069589}, {\"Minimum posts\": 97, \"Accuracy\": 99.70873786407766}, {\"Minimum posts\": 99, \"Accuracy\": 99.6116504854369}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfg = pd.DataFrame(columns = [\"Minimum posts\", \"Accuracy\"])\n",
    "dfg[\"Minimum posts\"] = minposts\n",
    "dfg[\"Accuracy\"] = accuracies\n",
    "\n",
    "base = alt.Chart(dfg).mark_point(fill = \"black\", stroke = \"black\").encode(\n",
    "    x = alt.X(\"Minimum posts\"),\n",
    "    y = alt.Y(\"Accuracy\", title = \"Accuracy (%)\").scale(zero = False)\n",
    ").properties(\n",
    "    width = 800,\n",
    "    height = 200\n",
    ")\n",
    "\n",
    "alt.layer(base.mark_line(color = \"grey\"), base).configure_axis(titleFontSize = 16, labelFontSize = 16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
